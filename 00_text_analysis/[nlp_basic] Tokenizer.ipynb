{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWqBWXSNBNVyhnGQ2G/ixP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KdY96t1sn-Yt","executionInfo":{"status":"ok","timestamp":1668068937875,"user_tz":-60,"elapsed":1615,"user":{"displayName":"Songyi Han","userId":"11159266753548301334"}}},"outputs":[],"source":["import nltk"]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"id":"1HOs3tRiqtqA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenizer \n","Tokenization is the process of splitting a text on punctuation into\n","smaller units. Each of these smaller units is called a token.\n","  \n","**Why do we need to tokenize a text?**\n","\n","Tokenization is a necessary step before our data are fed into a model, in other words into a machine learning\n","algorithm. This means that our data need to be transformed into a format that will allow the model to\n","understand it. This format is the tokens encoded into numbers. Splitting a text into tokens allows for their\n","conversion into numerical vectors.\n","\n","* TOKENIZE A TEXT INTO SENTENCES\n","* TOKENIZE A TEXT INTO WORD\n","* REGEX TOKENIZER"],"metadata":{"id":"cpLdT1Awpu88"}},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize"],"metadata":{"id":"u2Eo8Ns1p73W","executionInfo":{"status":"ok","timestamp":1668069012697,"user_tz":-60,"elapsed":298,"user":{"displayName":"Songyi Han","userId":"11159266753548301334"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# TOKENIZE A TEXT INTO SENTENCES\n","text = \"I'm making coffee. Would you like one?\"\n","tokens = sent_tokenize(text)\n","tokens"],"metadata":{"id":"5QbJbhOiqG_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TOKENIZE A SENTENCE INTO INDIVIDUAL ITEMS"],"metadata":{"id":"K2lBbQnnqaKC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize \n","text = \"I'm making coffee.\"\n","tokens = word_tokenize(text) \n","tokens "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X3c7KXlPqg-g","executionInfo":{"status":"ok","timestamp":1668069128753,"user_tz":-60,"elapsed":473,"user":{"displayName":"Songyi Han","userId":"11159266753548301334"}},"outputId":"579f6699-1b72-4506-b1b9-ed93000f35a5"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I', \"'m\", 'making', 'coffee', '.']"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# RegexpTokenizer\n","from nltk.tokenize import RegexpTokenizer\n","\n","tokens = RegexpTokenizer('\\w+')\n","print(tokens.tokenize(\"I can't COME NOW.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3c7S8tP9qlgk","executionInfo":{"status":"ok","timestamp":1668069409436,"user_tz":-60,"elapsed":215,"user":{"displayName":"Songyi Han","userId":"11159266753548301334"}},"outputId":"4cf57b63-837c-40d1-cc03-5db6dd923bd6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'can', 't', 'COME', 'NOW']\n"]}]},{"cell_type":"code","source":["tokens= RegexpTokenizer('\\w+|\\S')\n","print(tokens.tokenize(\"I can’t COME NOW.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kav4JEakrz50","executionInfo":{"status":"ok","timestamp":1668069455166,"user_tz":-60,"elapsed":6,"user":{"displayName":"Songyi Han","userId":"11159266753548301334"}},"outputId":"81f9d903-1f6a-48ec-d940-085bb8eea0b4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'can', '’', 't', 'COME', 'NOW', '.']\n"]}]},{"cell_type":"code","source":["# WordPunktTokenizer\n","from nltk.tokenize import WordPunctTokenizer\n","text=\"p.s. I'd love to come!\"\n","print(WordPunctTokenizer().tokenize(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sldLtJKor_Cm","executionInfo":{"status":"ok","timestamp":1668069545848,"user_tz":-60,"elapsed":558,"user":{"displayName":"Songyi Han","userId":"11159266753548301334"}},"outputId":"367044e0-f601-4354-d50a-1f69e60f98dd"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['p', '.', 's', '.', 'I', \"'\", 'd', 'love', 'to', 'come', '!']\n"]}]},{"cell_type":"code","source":["# Whitespace tokenizer\n","from nltk.tokenize import WhitespaceTokenizer\n","text = 'Would you like to travel to New York?\\nThe city is expensive\\tbut it is amazing!'\n","print(WhitespaceTokenizer().tokenize(text))"],"metadata":{"id":"wsFfwzCcsVLb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TreeBankTokenizer\n","# The TreeBank Tokenizer treats as separate tokens the punctuation marks, special characters and hyphenated words.\n","# It also splits contractions, e.g. 'ca', \"n't\" < can’t since it contains rules for the English contractions.\n","from nltk.tokenize import TreebankWordTokenizer\n","text= \"If you think you can't keep up-to-date don't @do it! \"\n","print(TreebankWordTokenizer().tokenize(text))"],"metadata":{"id":"Da-1r87nsgYv"},"execution_count":null,"outputs":[]}]}