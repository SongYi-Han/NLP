{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UgM7VEXCdbTX"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset \n","from sklearn.metrics import confusion_matrix, accuracy_score\n","\n","import csv\n","import re\n","import string\n","from collections import defaultdict\n"]},{"cell_type":"markdown","metadata":{"id":"WEtmPp7epxEZ"},"source":["## 0. Load and inspect the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BTTm7E7hz51"},"outputs":[],"source":["url_train = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTOZ2rC82rhNsJduoyKYTsVeH6ukd7Bpxvxn_afOibn3R-eadZGXu82eCU9IRpl4CK_gefEGsYrA_oM/pub?gid=1863430984&single=true&output=tsv'\n","url_test = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vT-KNR9nuYatLkSbzSRgpz6Ku1n4TN4w6kKmFLkA6QJHTfQzmX0puBsLF7PAAQJQAxUpgruDd_RRgK7/pub?gid=417546901&single=true&output=tsv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5D8w9zd1c2IJ"},"outputs":[],"source":["from io import StringIO\n","import requests\n","\n","def load_dataset(url):\n","    r = requests.get(url)\n","    data = r.content.decode('utf8')\n","    df = pd.read_csv(StringIO(data), sep='\\t')\n","    df.columns = ['tweet', 'label']\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNg0hoBsc5wL"},"outputs":[],"source":["df_train = load_dataset(url_train)\n","df_test = load_dataset(url_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1636896260878,"user":{"displayName":"Songyi Han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11159266753548301334"},"user_tz":-60},"id":"knWhKtIkSeBn","outputId":"94728e4d-9c2e-47b9-c6f3-1c6b55d518b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["(52675, 2)\n","(13279, 2)\n","num of language in train set:  69\n"]}],"source":["print(df_train.shape)\n","print(df_test.shape)\n","print(\"num of language in train set: \", len(np.unique(df_train['label'])))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1636894845549,"user":{"displayName":"Songyi Han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11159266753548301334"},"user_tz":-60},"id":"XSvXb7uliOYV","outputId":"4bfec735-19d9-4e41-8ae9-1ef66a81958c"},"outputs":[{"data":{"text/plain":["<bound method NDFrame.head of                                                    tweet label\n","0      يا من أناديها ويخنقني البكاء  ويكاد صمت الدمع ...    ar\n","1      فيه فرق بين اهل غزة اللى مطحونين من ناحيتين وب...    ar\n","2      ﻋﻦ ﺍﻟﻠﺤﻈﺔ اﻟﺤﻠﻮﺓﺓ ﺍﻟﻠﻲ ﺑﺘﻐﻤﺾ ﻓﻴﻬﺎ ﻋﻴﻨﻴﻚ ﺑﺘﻔﻜﺮ ...    ar\n","3                                      يا ابو سلو عرفتني    ar\n","4      ب50 ريال أكفل معتمر في رمضان ، ولك بإذن الله م...    ar\n","...                                                  ...   ...\n","42093                        그 내가 드래곤 라이딩에 대한 소설 읽어본적 있음    ko\n","42094              우주를 멸망시킬뻔한사람과 이기도록 백업을받는사람을 상대로 어떻게이겨    ko\n","42095                        다메다욬ㅋㅋㅋㅋㅋ그랫다간 배고파스주글거같은(파스스    ko\n","42096                           in_saeng_owo 아뇨 제가 경험한일임    ko\n","42097                                          osy9611 넵    ko\n","\n","[42041 rows x 2 columns]>"]},"execution_count":112,"metadata":{},"output_type":"execute_result"}],"source":["df_train.head"]},{"cell_type":"markdown","metadata":{"id":"b2jw5P5jGIvR"},"source":["## 1. Data preprocessing \n","This includes balancing the distribution of dataset, matching labels in both training and test dataset, normaliaing and splitting the data into training, validation, test dataset "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8rYsVJzTKs_"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","class Preprocessor:\n","  def __init__(self, df_train, df_test):\n","    self.df_train = df_train\n","    self.df_test = df_test\n","    self.df_train_bal = None\n","    self.df_test_matched = None\n","  \n","  def balancing(self):\n","    n_labels = self.df_train['label'].value_counts()\n","    rare_labels = [l for l in n_labels[n_labels <= 10].index]\n","    self.df_train_bal = self.df_train[df_train.label.isin(rare_labels) == False]\n","\n","  def match_labels(self):\n","    labels_train = self.df_train.label.unique()\n","    self.df_test_matched = df_test[self.df_test.label.isin(labels_train) == True]\n","\n","  def normalize(self, df_data):\n","    texts = list(df_data['tweet'])\n","    norm_texts = []\n","    for text in texts:\n","      text = text.lower()\n","      text = re.sub(r'\\n', '', text)\n","      text = re.sub(r'(\\s\\d+)','',text) \n","      text = re.sub(r'([.,!?@#])', '', text)\n","      norm_texts.append(text)  \n","    df_data.loc[:,'tweet'] = norm_texts\n","    return df_data\n","\n","  def split_train(self, ratio):\n","    df_train, df_val = train_test_split(self.df_train, test_size=ratio)\n","    return df_train, df_val\n","\n","  def preprocess(self):\n","    self.balancing()\n","    self.match_labels()\n","    self.df_train = self.normalize(self.df_train_bal)\n","    self.df_test = self.normalize(self.df_test_matched)\n","    df_train, df_val = self.split_train(0.8)\n","    return df_train, df_val, self.df_test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1636906122803,"user":{"displayName":"Songyi Han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11159266753548301334"},"user_tz":-60},"id":"qteKLQf7ll9s","outputId":"948941e5-d09c-43f7-d8e0-6e3a58e77583"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1781: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self.obj[item_labels[indexer[info_axis]]] = value\n"]}],"source":["preprocessor = Preprocessor(df_train, df_test)\n","df_train, df_val, df_test = preprocessor.preprocess()\n"]},{"cell_type":"markdown","metadata":{"id":"O1gHIDt0l6ML"},"source":["## 2. Vectorize the tweet text & Label encoding\n","\n","We built a feature vector by counting the 100 most frequently appearing bigram in given text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNikiyV9l5wz"},"outputs":[],"source":["from nltk import ngrams\n","from sklearn.preprocessing import LabelEncoder\n","\n","class Vectorize_and_Encoding:\n","  def __init__(self, df_train, df_val, df_test):\n","    self.df_total = None\n","    self.df_train = df_train\n","    self.df_val = df_val\n","    self.df_test = df_test\n","    self.bigram_by_lan = {}\n","    self.sorted_unique_bigrams = []\n","    self.num_features = None\n","    self.bigram_to_idx = None\n","\n","  def create_bigrams(self, word):\n","    bigrams = ngrams(word, 2)\n","    for bigram in bigrams:\n","      res.append(bigram)\n","    return res\n","\n","  def get_bigrams(self, tweet):\n","    return [i+j for i,j in zip(tweet, tweet[1:])]\n","    \n","  def count_frequent_bigram(self, X_train, y_train):\n","    tweet_by_lan = {}\n","    labels = np.unique(y_train)\n","    for label in labels:\n","      tweet_by_lan[label] = [] \n","        \n","    for tweet, label in zip (list(X_train),list(y_train)):\n","      tweet_by_lan[label].append(tweet)\n","\n","    for label in labels:\n","      tweets = tweet_by_lan[label]\n","      bigram_freqs = {}\n","      for tweet in tweets:\n","        for bigram in self.get_bigrams(tweet):\n","          if bigram in bigram_freqs:\n","            bigram_freqs[bigram] += 1\n","          else:\n","            bigram_freqs[bigram] = 1\n","      sorted_bigrams = sorted([(bigram, freq) for bigram, freq in bigram_freqs.items()], key=lambda x: x[1], reverse=True)\n","      self.bigram_by_lan[label] = sorted_bigrams[:100]\n","\n","    for label, bigrams in self.bigram_by_lan.items():\n","      for bigram, freq in bigrams:\n","        self.sorted_unique_bigrams.append(bigram)\n","\n","    # create vocaburay    \n","    self.sorted_unique_bigrams = set(self.sorted_unique_bigrams)\n","    self.bigram_to_idx = {bigram: i for i, bigram in enumerate(self.sorted_unique_bigrams)}\n","    self.num_features = len(self.sorted_unique_bigrams)\n","\n","  def build_feature_vector(self, X_train):\n","    X_vec = []\n","    for tweet in X_train: \n","      feature_vector = np.zeros(self.num_features)\n","      for bigram in self.get_bigrams(tweet):\n","        if bigram in self.bigram_to_idx:\n","          feature_vector[self.bigram_to_idx[bigram]] += 1\n","      X_vec.append(feature_vector)\n","    X_vec = pd.DataFrame(X_vec)\n","    return X_vec\n","\n","  def vectorize(self):\n","    self.count_frequent_bigram(self.df_train['tweet'], self.df_train['label'])\n","    X_train = self.build_feature_vector(self.df_train['tweet'])\n","    X_val = self.build_feature_vector(self.df_val['tweet'])\n","    X_test = self.build_feature_vector(self.df_test['tweet'])\n","\n","    return X_train, X_val, X_test\n","\n","  def encoding(self):\n","    frame = [self.df_train, self.df_val, self.df_test]\n","    self.df_total = pd.concat(frame, ignore_index = True)\n","    le = LabelEncoder()\n","    le.fit(self.df_total['label'])\n","    y_train = le.transform(self.df_train['label'])\n","    y_val = le.transform(self.df_val['label'])\n","    y_test = le.transform(self.df_test['label'])\n","    num_classes = len(np.unique(self.df_total['label']))\n","    return y_train, y_val, y_test, num_classes \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNiFSrGiQWqP"},"outputs":[],"source":["# prepare dataset \n","\n","ve = Vectorize_and_Encoding(df_train,df_val,df_test)\n","X_train, X_val, X_test = ve.vectorize()\n","y_train, y_val, y_test, num_classes = ve.encoding()\n","\n","num_features = len(X_train.iloc[0])\n"]},{"cell_type":"markdown","metadata":{"id":"aJVBpIEIMa4f"},"source":["### 3. Training Model\n","\n","find the optimal model architecture and training regime for your CNN classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3D8srKQRDWa"},"outputs":[],"source":["class ConvClassifier(nn.Module):\n","    def __init__(self, kernel, stride, pooling, padding, lin_in, num_classes):\n","        super(ConvClassifier, self).__init__()\n","        self.convl1 = nn.Sequential(\n","            nn.Conv1d(1, 100, kernel_size=kernel, stride=stride, padding=padding),\n","            nn.ELU(),\n","            nn.MaxPool1d(kernel_size=pooling, stride=pooling))\n","        self.convl2 = nn.Sequential(\n","            nn.Conv1d(100, 1, kernel_size=kernel, stride=stride, padding=padding),\n","            nn.ELU(),\n","            nn.MaxPool1d(kernel_size=pooling, stride=pooling))\n","        self.lin = nn.Linear(lin_in, num_classes)\n","        \n","    def forward(self, x):\n","        out1 = self.convl1(x)\n","        out2 = self.convl2(out1)\n","        out2 = out2.reshape(out2.size(0), -1)\n","        out = self.lin(out2)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfxRMYqOAp_X"},"outputs":[],"source":["def batch_generator(X_train, y_train, batch_size):\n","    # initialize the index\n","    start= 0\n","    end= batch_size\n","    # generate batch based on batch size\n","    while end <= len(X_train):\n","        yield (X_train.iloc[start:end].values, y_train[start:end])\n","        start, end = start+batch_size, end+batch_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXulnHpQRBv7"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","# Loss and optimizer\n","device = 'cpu'\n","num_epochs = 3\n","num_features = len(X_train.iloc[0])\n","loss_fun = nn.CrossEntropyLoss()\n","\n","hypers = [\n","          {'kernel': 3, 'stride': 1, 'pooling':2, 'padding':0, 'batch_size': 64},\n","          {'kernel': 3, 'stride': 2, 'pooling':2, 'padding':2, 'batch_size': 128},\n","          {'kernel': 5, 'stride': 1, 'pooling':2, 'padding':0, 'batch_size': 64},\n","          {'kernel': 5, 'stride': 1, 'pooling':2, 'padding':0, 'batch_size': 128},\n","          {'kernel': 5, 'stride': 2, 'pooling':2, 'padding':2, 'batch_size': 256}\n","] \n","\n","res = []\n","models = []\n","\n","for hyp in hypers:\n","    kernel = hyp['kernel']\n","    stride = hyp['stride']\n","    pooling = hyp['pooling']\n","    padding = hyp['padding']\n","    batch_size = hyp['batch_size']\n","\n","    # calculate chnnel dimension based on hyperparameters \n","    out1_channel = ((((num_features+2*padding)-kernel) // stride)+1) // pooling\n","    out2_channel = ((((out1_channel+2*padding)-kernel) // stride)+1) // pooling\n","\n","    #Define model with new hyperparameter and optimizer      \n","    model = ConvClassifier(kernel=kernel, stride=stride, pooling=pooling, padding=padding, lin_in = out2_channel, num_classes=num_classes)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    \n","    #Train the model\n","    for epoch in range(num_epochs):\n","      print('--- !! Start training cnn model with hyperprameters condition - {}'.format(hyp))\n","      for i, (batch_x, batch_y) in enumerate(batch_generator(X_train, y_train, batch_size)):\n","        #reset the gradients\n","        optimizer.zero_grad()\n","          \n","        # fit the model\n","        outputs = []\n","        for tweet in batch_x:                 \n","          output = model(torch.FloatTensor([[tweet]]).to(device))\n","          outputs.append(output)\n","        outputs = torch.stack(outputs)\n","\n","        #compute the loss\n","        loss = loss_fun(torch.squeeze(outputs), torch.squeeze(torch.LongTensor(batch_y)).to(device))\n","            \n","        #take gradient\n","        loss.backward()\n","        optimizer.step()\n","                \n","        print ('Epoch {}/{} || batch {}/{} ...... Loss : {:.4f}'.format(epoch+1, num_epochs, i+1, int((len(X_train)/batch_size)+2), loss.item()))\n","    models.append(model)\n","\n","    # evaluate model     \n","    print('---- ## Model validation ')\n","    y_preds = []\n","    for x in X_train.values:\n","        pred = model(torch.FloatTensor([[x]]).to(device))\n","        label_pred = list(pred[0]).index(max(list(pred[0])))\n","        y_preds.append(label_pred)\n","    res.append((accuracy_score(y_train, y_preds), f1_score(y_train, y_preds, average='macro')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzhiC-H4VlBW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":324,"status":"ok","timestamp":1636908114983,"user":{"displayName":"Songyi Han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11159266753548301334"},"user_tz":-60},"id":"uR6N1ysVyPMW","outputId":"34b558de-6da4-4215-ba43-c1381f9321cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["====================================================================================================\n","hyperparameter condition: {'kernel': 3, 'stride': 1, 'pooling': 2, 'padding': 0, 'batch_size': 64}\n","Accuracy is 0.8893, f1 max is 0.3432\n","====================================================================================================\n","hyperparameter condition: {'kernel': 3, 'stride': 2, 'pooling': 2, 'padding': 2, 'batch_size': 128}\n","Accuracy is 0.4481, f1 max is 0.0344\n","====================================================================================================\n","hyperparameter condition: {'kernel': 5, 'stride': 1, 'pooling': 2, 'padding': 0, 'batch_size': 64}\n","Accuracy is 0.8843, f1 max is 0.3273\n","====================================================================================================\n","hyperparameter condition: {'kernel': 5, 'stride': 1, 'pooling': 2, 'padding': 0, 'batch_size': 128}\n","Accuracy is 0.8548, f1 max is 0.3215\n","====================================================================================================\n","hyperparameter condition: {'kernel': 5, 'stride': 2, 'pooling': 2, 'padding': 2, 'batch_size': 256}\n","Accuracy is 0.5635, f1 max is 0.1051\n"]}],"source":["# report the evaluation results per parameters\n","for i in range(len(hypers)):\n","  print(100*'=')\n","  print('hyperparameter condition: {}'.format(hypers[i]))\n","  print('Accuracy is {:.4f}, f1 max is {:.4f}'.format(res[i][0], res[i][1]))"]},{"cell_type":"markdown","metadata":{"id":"NFiLTIqXJh-M"},"source":["# 4. Test\n","Take the best performing model and evaluate it on the test set. Report the result with confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14852,"status":"ok","timestamp":1636908151324,"user":{"displayName":"Songyi Han","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11159266753548301334"},"user_tz":-60},"id":"PzXybnu2EE3O","outputId":"8dd6f5ba-311e-4cd7-9389-e6eb778f07d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Our classifier with optimal hyperparameter\n","Accuracy is 0.4501, fi_macro score is 0.0365\n"]}],"source":["opt_hyp = {'kernel': 3, 'stride': 1, 'pooling': 2, 'padding': 0, 'batch_size': 64}\n","opt_model = models[0]\n","\n","y_preds = []\n","\n","for tweet in X_test.values:\n","  y_pred = opt_model(torch.FloatTensor([[tweet]]).to(device))\n","  y_pred = list(y_pred[0]).index(max(list(y_pred[0])))\n","  y_preds.append(label_pred)\n","\n","accuracy = accuracy_score(y_test, y_preds)\n","f1_macro = f1_score(y_test, y_preds, average='macro')\n","\n","print(\"Our classifier with optimal hyperparameter\")\n","print(\"Accuracy is {:.4f}, fi_macro score is {:.4f}\".format(accuracy, f1_macro))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5B4hnRXVHRY"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ex03_cnn.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
