{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Sequence and Sentiment Classification using Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part1. Named Entity Recognition using BERT\n",
    "\n",
    "Implement a named entity recognition system for your chosen language. Use HuggingFace’s BertForTokenClassification-class and\n",
    "initialize it with a pretrained Hugging Face BERT-base model of your chosen language. This HuggingFace guide for fine-tuning\n",
    "serves as a good starting point. Before passing the data to the model, you need to encode it using a HuggingFace tokenizer. Use\n",
    "the tokenizer corresponding to your BERT model. When provided with the right arguments, the tokenizer can also pad and truncate\n",
    "the input.\n",
    "You can reduce the amount of code for this exercise by using the Trainer class explained at the bottom of the HuggingFace guide.\n",
    "You will create 3 fine-tuned versions of the system:\n",
    "1. Fine-tuned with 1’000 sentences\n",
    "2. Fine-tuned with 3’000 sentences\n",
    "3. Fine-tuned with 3’000 sentences and frozen embeddings\n",
    "Let each fine-tuned model predict on the evaluation set to compute f1-micro and f1-macro scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset polyglot_ner (C:\\Users\\songy\\.cache\\huggingface\\datasets\\polyglot_ner\\de\\1.0.0\\bb2e45c90cd345c87dfd757c8e2b808b78b0094543b511ac49bc0129699609c1)\n",
      "Reusing dataset polyglot_ner (C:\\Users\\songy\\.cache\\huggingface\\datasets\\polyglot_ner\\de\\1.0.0\\bb2e45c90cd345c87dfd757c8e2b808b78b0094543b511ac49bc0129699609c1)\n",
      "Reusing dataset polyglot_ner (C:\\Users\\songy\\.cache\\huggingface\\datasets\\polyglot_ner\\de\\1.0.0\\bb2e45c90cd345c87dfd757c8e2b808b78b0094543b511ac49bc0129699609c1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'lang', 'words', 'ner'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'lang', 'words', 'ner'],\n",
      "    num_rows: 3000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'lang', 'words', 'ner'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load dataset with huggingface load_dataset \n",
    "train1 = datasets.load_dataset('polyglot_ner', 'de', split='train[:1000]')\n",
    "train2 = datasets.load_dataset('polyglot_ner', 'de', split='train[:3000]')\n",
    "test = datasets.load_dataset('polyglot_ner', 'de', split='train[-2000:]')\n",
    "\n",
    "print(train1)\n",
    "print(train2)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC', 'PER', 'O', 'ORG'}\n"
     ]
    }
   ],
   "source": [
    "# build a vocaburary set to map ner tags to labels\n",
    "\n",
    "ner_vocab = set()\n",
    "\n",
    "for row in train1['ner']:\n",
    "    for tag in row:\n",
    "        ner_vocab.add(tag)\n",
    "\n",
    "print(ner_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': 0, 'PER': 1, 'O': 2, 'ORG': 3}\n"
     ]
    }
   ],
   "source": [
    "# label encoding  \n",
    "\n",
    "tags_to_labels = {tag: i for i, tag in enumerate(ner_vocab)}\n",
    "\n",
    "print(tags_to_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 154 158\n"
     ]
    }
   ],
   "source": [
    "max_len_1 = max([len(row['words']) for row in train1])\n",
    "max_len_2 = max([len(row['words']) for row in train2])\n",
    "max_len_3 = max([len(row['words']) for row in test])\n",
    "\n",
    "print(max_len_1, max_len_2, max_len_3)\n",
    "\n",
    "max_len = max_len_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset):\n",
    "    encoded_dataset = []\n",
    "    for words, ners in zip(dataset['words'], dataset['ner']):\n",
    "        enc = tokenizer(words, return_tensors=\"pt\", padding='max_length', max_length=max_len, truncation=True, is_split_into_words=True)\n",
    "        enc['labels'] = torch.zeros(1, max_len, dtype=torch.long)\n",
    "        for i, tag in enumerate(ners[:max_len]):\n",
    "            enc['labels'][0][i] = tags_to_labels[tag]\n",
    "        for key in enc:\n",
    "            enc[key] = torch.squeeze(enc[key])\n",
    "        encoded_dataset.append(enc)\n",
    "        \n",
    "    return encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the two training sets and the evaluation set\n",
    "\n",
    "enc_train1 = encode_dataset(train1)\n",
    "enc_train2 = encode_dataset(train2)\n",
    "enc_test = encode_dataset(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_weights(model):\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define a model \n",
    "model = BertForTokenClassification.from_pretrained('bert-base-german-cased', num_labels=len(ner_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# parameters 1 : fine-tuned with 1000 sentence\n",
    "\n",
    "train_param1 = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    output_dir='result1',\n",
    "    logging_dir='logs1',\n",
    "    no_cuda=False,  \n",
    ")\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_param1,\n",
    "    train_dataset=enc_train1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# parameters 2 : fine-tuned with 3000 sentence\n",
    "\n",
    "train_param2 = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    output_dir='result2',\n",
    "    logging_dir='logs2',\n",
    "    no_cuda=False,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_param2,\n",
    "    train_dataset=enc_train2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# parameters 3 : fine-tuned with 3000 sentences and frozen embeddings\n",
    "\n",
    "train_param3 = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    output_dir='results3',\n",
    "    logging_dir='logs3',\n",
    "    no_cuda=False,\n",
    ")\n",
    "\n",
    "trainer3 = Trainer(\n",
    "    model=freeze_weights(model),\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_param2,\n",
    "    train_dataset=enc_train2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 09:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.155072021484375, metrics={'train_runtime': 567.227, 'train_samples_per_second': 1.763, 'train_steps_per_second': 0.441, 'total_flos': 80636004048000.0, 'train_loss': 1.155072021484375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 17:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds1 = trainer1.predict(enc_test)\n",
    "\n",
    "# play around with preds - preds.label_ids, preds.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 37:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to result2\\checkpoint-500\n",
      "Configuration saved in result2\\checkpoint-500\\config.json\n",
      "Model weights saved in result2\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in result2\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in result2\\checkpoint-500\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.6657684529622396, metrics={'train_runtime': 2238.4188, 'train_samples_per_second': 1.34, 'train_steps_per_second': 0.335, 'total_flos': 235783758672000.0, 'train_loss': 0.6657684529622396, 'epoch': 1.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 16:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds2 = trainer2.predict(enc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds3 = trainer3.predict(enc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "num_words_list = [len(words) for words in test['words']]\n",
    "\n",
    "def print_f1score(preds):\n",
    "\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for i, (num_words, label_ids, label_preds) in enumerate(zip(num_words_list, preds.label_ids, preds.predictions)):\n",
    "        label_true = label_ids[:num_words]\n",
    "        labels_list.extend(label_true)\n",
    "        preds = label_preds[:num_words]\n",
    "        preds = preds.argmax(-1)\n",
    "        preds_list.extend(preds)\n",
    "        \n",
    "    f1_micro = f1_score(labels_list, preds_list, average='micro')\n",
    "    f1_macro = f1_score(labels_list, preds_list, average='macro')\n",
    "\n",
    "    print(f\"fl_micro is {f1_micro} and f1_macro is {f1_macro}\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "result of parameters 1) fine-tuned with 1000 sentence\n",
      "fl_micro is 0.7013919698314108 and f1_macro is 0.22175120580593963\n",
      "============================================================\n",
      "result of parameters 2) fine-tuned with 3000 sentence\n",
      "============================================================\n",
      "result of parameters 3) fine-tuned with 3000 sentence and frozen embeddings\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"result of parameters 1) fine-tuned with 1000 sentence\")\n",
    "print_f1score(preds1)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"result of parameters 2) fine-tuned with 3000 sentence\")\n",
    "#print_f1score(preds2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"result of parameters 3) fine-tuned with 3000 sentence and frozen embeddings\")\n",
    "#print_f1score(preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
