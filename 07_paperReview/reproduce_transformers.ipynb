{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nSmos9XliFZ"
      },
      "source": [
        "<h1><center>\n",
        "    Implementing transformer architectures. <br/>\n",
        "</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_10tVUdliFo"
      },
      "source": [
        "Objective : Implement the feed-forward pass of the original transformer network using only numpy (without machine learning frameworks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJQkcIuNliFp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Test on a single array:\n",
        "forward_pass_array = np.array([101, 400, 500, 600, 107, 102])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82CSrksLliFs"
      },
      "source": [
        "## Step 1.1\n",
        "Implement the sinus/cosinus positional encoding and token embedding used in the original paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762).  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Defining source vocabulary, target vocabulary tokens, and corresponding word embeddings (b/w 0 and 1)\n",
        "'''\n",
        "\n",
        "emb_size = 512 \n",
        "\n",
        "src_vocab = np.array([101,102,103,104,105,106,107,108,109,110,400,500,600])\n",
        "tgt_vocab = np.array([i for i in range(700,720)])\n",
        "\n",
        "np.random.seed(42)\n",
        "src_emb = np.random.rand(len(src_vocab),emb_size)\n",
        "np.random.seed(43)\n",
        "tgt_emb = np.random.rand(len(tgt_vocab),emb_size)"
      ],
      "metadata": {
        "id": "kxlnjxvvbwct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5F2-NzdliFt"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Input = max_seq_len=6, pos_encod_len (d_model) = 128\n",
        "To do = check if pos_encod_len = 128 or 512.\n",
        "      = check with some python package \n",
        "'''\n",
        "\n",
        "def positional_encoding(pos_encod_len, max_seq_len=6):\n",
        "    MAX_SEQ_LEN = max_seq_len # maximum length of a sentence\n",
        "    d_model = pos_encod_len # word embedding (and positional encoding) dimensions\n",
        "    PE = np.zeros((MAX_SEQ_LEN, d_model))\n",
        "\n",
        "    for pos in range(MAX_SEQ_LEN):\n",
        "        for i in range(d_model//2):\n",
        "            theta = pos / (10000 ** ((2*i)/d_model))\n",
        "            PE[pos, 2*i] = math.sin(theta)\n",
        "            PE[pos, 2*i+1] = math.cos(theta)\n",
        "    return PE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to get word embeddings from token_list and vocab. \n",
        "Input = token_array: Array of input tokens in text. \n",
        "        vocab: vocabulary for tokens \n",
        "        vocab_emb: Vocabulary embedding\n",
        "'''\n",
        "\n",
        "def generate_word_embedding(token_array, vocab, vocab_emb):\n",
        "    token_ids = np.array([np.where(src_vocab==token)[0][0] for token in list(token_array)])\n",
        "    emb_matrix = vocab_emb[token_ids,:]\n",
        "    return emb_matrix"
      ],
      "metadata": {
        "id": "sRlp56rIEz4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4UTsLSNliFt"
      },
      "source": [
        "## Step 1.2\n",
        "Implement a dense layer with the number of hidden units as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdU_VWUoliFu"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Simple NN Layer - For FFNN \n",
        "X : Input data matrix \n",
        "hidden_units : Neurons \n",
        "'''\n",
        "\n",
        "def dense_layer(X, hidden_units):\n",
        "    W = np.random.rand(X.shape[1], hidden_units)\n",
        "    b = np.zeros((1, hidden_units))\n",
        "    return np.dot(X, W) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qas2R8NEliFu"
      },
      "source": [
        "## Step 1.3\n",
        "Implement all activation function such that they are compatible with the dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MlHN4srliFv"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Implement Sigmoid, TanH, Relu\n",
        "'''\n",
        "\n",
        "def relu(X):\n",
        "    return np.maximum(0, X)\n",
        "\n",
        "def sigmoid(X):\n",
        "    return 1/(1 + np.exp(-X))\n",
        "\n",
        "def tanh(X):\n",
        "    return np.tanh(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q78GbdcliFy"
      },
      "source": [
        "## Step 1.4\n",
        "Implement dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGe4vg7lliFz"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function to implement dropout. \n",
        "\n",
        "hidden_layer = Weight Matrix \n",
        "prob = probability to keep neurons (0.8 = 80% of neurons will be kept)\n",
        "\n",
        "Output = Modified weights. \n",
        "\n",
        "\n",
        "Places dropout used - \n",
        "1. FFNN \n",
        "2. Residual Connection (before passing it to layernorm)\n",
        "3. Self Attention\n",
        "4. Encoder Decoder Attention\n",
        "'''\n",
        "def dropout(hidden_layer, prob=0.9):\n",
        "    mask = np.random.binomial(1, prob, size=hidden_layer.shape) / prob\n",
        "    out = hidden_layer * mask\n",
        "    return out.reshape(hidden_layer.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmNT6o84liFw"
      },
      "source": [
        "## Step 1.5\n",
        "Implement the skip (residual) connections - Implemented after dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Residual Connection - Each sub-layer (self-attention, ffnn) in each encoder/decoder has a \n",
        "                      residual connection around it, and is followed by a layer-normalization step.\n",
        "'''\n",
        "\n",
        "'''\n",
        "Input: X = matrix from previous layer to output (input to out)\n",
        "       out = output from previous layer. number of columns in X should be divisible by number of columns in out.\n",
        "       \n",
        "Output: Modified matrix with dropout\n",
        "'''\n",
        "def residual_connection(X, out):\n",
        "    return dropout(X + out)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N0cHsXjksNIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSll0qwbliFx"
      },
      "source": [
        "## Step 1.6\n",
        "Implement layer normalization - Implemented after dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input = weight matrix\n",
        "\n",
        "Output = modified weight matrix after dropout. \n",
        "'''\n",
        "\n",
        "def layer_norm(X):\n",
        "    eps = 1e-5\n",
        "    mean = X.mean(axis=1)\n",
        "    std = X.std(axis=1)\n",
        "    denominator = np.sqrt(std**2 + eps)[:,None]\n",
        "    numerator = X - mean[:,None]\n",
        "    layer_norm_score = numerator/denominator\n",
        "    return layer_norm_score"
      ],
      "metadata": {
        "id": "qRNQIX7hsR8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHA1yNk_liFz"
      },
      "source": [
        "## Step 1.7\n",
        "Implement the attention mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzpUlya-liF0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function to get query, key and value matrices. \n",
        "Input = Embedding Matrix X : (seq_len*embedding_size), req_dim : required dimension of q,k,v. (generally 64) \n",
        "Output = q,k,v matrices : dim = seq_len*req_dim\n",
        "'''\n",
        "\n",
        "def calculate_qkv(X, req_dim=64):\n",
        "    Wq = np.random.rand(X.shape[1],req_dim)\n",
        "    Wk = np.random.rand(X.shape[1],req_dim)\n",
        "    Wv = np.random.rand(X.shape[1],req_dim)\n",
        "    \n",
        "    query = np.matmul(X,Wq)\n",
        "    key = np.matmul(X,Wk)\n",
        "    value = np.matmul(X,Wv)\n",
        "    \n",
        "    return query, key, value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to fill mask - required for masked attention\n",
        "'''\n",
        "def fill_mask(in_array, mask_val):\n",
        "    array_copy = in_array.copy()\n",
        "    array_copy = np.tril(array_copy)\n",
        "    array_copy[np.where(array_copy==0)] = mask_val\n",
        "    return array_copy"
      ],
      "metadata": {
        "id": "M-O4uYOYWAeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to return softmax along rows. \n",
        "Numerically stable softmax\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    max_val = np.max(x,axis=1,keepdims=True) #returns max of each row and keeps same dims\n",
        "    e_x = np.exp(x - max_val) #subtracts each row with its max value\n",
        "    sum_val = np.sum(e_x,axis=1,keepdims=True) #returns sum of each row and keeps same dims\n",
        "    f_x = e_x / sum_val \n",
        "    return f_x\n"
      ],
      "metadata": {
        "id": "6PuRL5TaWDa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Self Attention = softmax((Q*K_transpose)/(sqrt(d_k)))*V\n",
        "Use Mask for self-attention in decoder\n",
        "\n",
        "Input = Query Matrix (Q) , Key Matrix (K), Value Matrix (V)\n",
        "Dimensison of Q,K,V = sequence_length (n) * d_k (64 in the paper, dependes on Wq, Wk, Wv) \n",
        "Output = Self Attention Matrix : row r = attention scores over all the words for word r. \n",
        "         Dimension of Output = sequence_length (n) * 64\n",
        "\n",
        "q = n * 64 \n",
        "k = n * 64\n",
        "n = n * 64\n",
        "\n",
        "qkt = n*n\n",
        "softmax(qkt) * v = n*64\n",
        "'''\n",
        "\n",
        "def self_attention(query, key, value, mask=False):\n",
        "    d_k = query.shape[1]\n",
        "    scores = np.matmul(query, key.transpose())/np.sqrt(d_k)\n",
        "    if mask:\n",
        "        scores = fill_mask(scores,mask_val=-np.inf)\n",
        "    scores_softmax = softmax(scores)\n",
        "    out_scores = dropout(np.matmul(scores_softmax,value))\n",
        "    return out_scores"
      ],
      "metadata": {
        "id": "mw44IvkdWGMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to get multihead attention scores\n",
        "\n",
        "Input = X: Word Embedding matrix (seq_len * embedding_size) or output from previous encoder \n",
        "        num_heads: heads required, \n",
        "        qkv_req_dim : required dimension (columns) of q,k,v \n",
        "        \n",
        "Outputs = multihead attention scores. (Dimension: sequence_len*512)\n",
        "'''\n",
        "\n",
        "def multihead_attention(X, num_heads, qkv_req_dim, mask=False):\n",
        "    ## initialize array to store attention scores calculated from each head\n",
        "    z_scores = np.zeros(shape = (X.shape[0], num_heads*qkv_req_dim), dtype = 'float')\n",
        "    ## Lists to store query, key, vector values \n",
        "    query_list = [] \n",
        "    key_list = [] \n",
        "    value_list = []\n",
        "    for head in range(num_heads):\n",
        "        q, k, v = calculate_qkv(X,qkv_req_dim)\n",
        "        query_list.append(q)\n",
        "        key_list.append(k)\n",
        "        value_list.append(v)\n",
        "        attn_scores = self_attention(q,k,v,mask)\n",
        "        \n",
        "        ## Copy scores to z_scores\n",
        "        start_idx = head*qkv_req_dim\n",
        "        end_idx = (head+1)*qkv_req_dim\n",
        "        z_scores[:,start_idx:end_idx] = attn_scores\n",
        "    \n",
        "    ## Iniitialize weight matrix to multiply with z_scores \n",
        "    w_o = np.random.rand(qkv_req_dim*num_heads, X.shape[1])\n",
        "    \n",
        "    multi_attn_score = np.matmul(z_scores, w_o)\n",
        "    return query_list, key_list, value_list, multi_attn_score"
      ],
      "metadata": {
        "id": "wS1ElFwdWIyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC8ioTqDliF0"
      },
      "source": [
        "## Step 1.8\n",
        "Implement the positonal feed-forward network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvhdenkSliF1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "FFNN - \n",
        "Input= X: Input Matrix \n",
        "       n_hidden_layers = Number of hidden layers \n",
        "       num_neuron_list = list of no. of neurons in each layer. \n",
        "       activation_fn = activation function - can be 'relu','sigmoid' or 'tanh'\n",
        "\n",
        "1. Dense_layer\n",
        "2. Dropout \n",
        "3. Activation function\n",
        "\n",
        "Add dropout\n",
        "Only need Forward Pass. \n",
        "Complete this\n",
        "'''\n",
        "\n",
        "def ffnn(X, n_hidden_layers=2, num_neuron_list=[512,512], activation_fn='relu'):\n",
        "    wt_matrices = [X]\n",
        "    for layer in range(n_hidden_layers):\n",
        "        dense = dense_layer(wt_matrices[layer],num_neuron_list[layer])\n",
        "        drop = dropout(dense)\n",
        "        if activation_fn == 'relu':\n",
        "            out = relu(drop)\n",
        "        elif activation_fn == 'tanh':\n",
        "            out = tanh(drop)\n",
        "        else:\n",
        "            out = sigmoid(drop)\n",
        "        wt_matrices.append(out)\n",
        "    return wt_matrices[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h45tqPBkliF1"
      },
      "source": [
        "## Step 1.9\n",
        "Implement the encoder attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GJCnSFUliF2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function to implement Encoder Attention. Refers to multihead attention function\n",
        "\n",
        "Input = X: Word Embedding matrix (seq_len * embedding_size) or output from previous encoder. \n",
        "        num_heads: Number of attention heads. \n",
        "        qkv_req_dim : Dimenstion of query, key, value matrices (only number of columns)\n",
        "\n",
        "Output = Multi head attention_scores (without masks)\n",
        "'''\n",
        "def encoder_attention(X,num_heads=8,qkv_req_dim=64):\n",
        "    q_list, k_list, v_list, attention_scores = multihead_attention(X,num_heads,qkv_req_dim)\n",
        "    return q_list, k_list, v_list, attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXPZ4TTuliF2"
      },
      "source": [
        "## Step 1.10\n",
        "Implement the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6HzpTsoliF2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function to create a single encoder. \n",
        "\n",
        "Input: word_emb: Word Embedding Matrix (Including Positional Encoding) or output from previous layer. \n",
        "       num_heads: Number of attention heads. \n",
        "       qkv_req_dim: Dimenstion of query, key, value matrices (only number of columns)\n",
        "\n",
        "Output: Final Scores, Queries, Keys, Values used in Multihead attention, \n",
        "        Processed Word Embedding Matrix after passing through Encoder\n",
        "\n",
        "Steps: \n",
        "1. Multihead Self-Attention Layer\n",
        "2. Residual Connection and LayerNorm (Add and Normalize)\n",
        "3. FFNN  \n",
        "4. Residual Connection and LayerNorm (Add and Normalize)\n",
        "'''\n",
        "\n",
        "def encoder(word_emb, num_heads=8, qkv_req_dim=64):\n",
        "    q_list, k_list, v_list, attn_scores = encoder_attention(word_emb, num_heads, qkv_req_dim)\n",
        "    layer_norm_scores = layer_norm(residual_connection(word_emb,attn_scores))\n",
        "    #print(\"Shape of layer_norm_scores: \" + str(layer_norm_scores.shape))\n",
        "    ffnn_scores = ffnn(layer_norm_scores)\n",
        "    #print(\"Shape of ffnn_scores: \" + str(ffnn_scores.shape))\n",
        "    final_scores = layer_norm(residual_connection(ffnn_scores,layer_norm_scores))\n",
        "    return q_list, k_list, v_list, final_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to create encoder stack. 6 in original implementation. \n",
        "\n",
        "Input: word_emb: Word Embedding Matrix (Including Positional Encoding)\n",
        "       stack_size: Number of encoders \n",
        "       num_heads: Number of attention heads. \n",
        "       qkv_req_dim: Dimenstion of query, key, value matrices (only number of columns)\n",
        "       \n",
        "Output: Final_Scores, Queries, Keys, Values from all encoder layers. \n",
        "'''\n",
        "def encoder_stack(word_emb, stack_size = 6, num_heads=8, qkv_req_dim=64):\n",
        "    ## list to store encoder outputs. \n",
        "    encoder_op_list = []\n",
        "    \n",
        "    ## Output of first encoder \n",
        "    encoder_op1 = encoder(word_emb, num_heads, qkv_req_dim)\n",
        "    \n",
        "    encoder_op_list.append(encoder_op1)\n",
        "    \n",
        "    ## Pass final scores from previous encoders as input to next encoder. \n",
        "    for i in range(1,stack_size):\n",
        "        encoder_op = encoder(encoder_op_list[i-1][-1], num_heads, qkv_req_dim)\n",
        "        encoder_op_list.append(encoder_op)\n",
        "    \n",
        "    return encoder_op_list"
      ],
      "metadata": {
        "id": "7AuYmbe0F9p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLc2Xhf4liF3"
      },
      "source": [
        "## Step 1.11\n",
        "Implement the decoder attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5feOdr7zliF3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Input = X: Output Word Embedding matrix (max_seq_len_decoder * embedding_size), or scores from previous decoder.\n",
        "        num_heads: heads required, \n",
        "        qkv_req_dim : required dimension (columns) of q,k,v \n",
        "        \n",
        "Output = Multi head attention_scores (with masks) and decoder query list (to be used for enc-dec attention)\n",
        "'''\n",
        "def decoder_self_attention(X,num_heads=8,qkv_req_dim=64,mask=True):\n",
        "    decoder_query_list, _, _, attention_scores = multihead_attention(X,num_heads,qkv_req_dim,mask=True)\n",
        "    return decoder_query_list, attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMv--8ACliF3"
      },
      "source": [
        "## Step 1.12\n",
        "Implement the encoder-decoder attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4IWC5lkliF4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Input = X: embedding matrix  \n",
        "        enc_scores: final scores from encoder layer. \n",
        "        dec_scores: scores coming after add and norm from same decoder. \n",
        "\n",
        "Output = Multihead Encoder-Decoder Attention scores. \n",
        "'''\n",
        "\n",
        "def encoder_decoder_attention(X, enc_scores, dec_scores, num_heads=8, qkv_req_dim=64,mask=False):\n",
        "    ## initialize array to store attention scores calculated from each head\n",
        "    z_scores = np.zeros(shape = (X.shape[0], num_heads*qkv_req_dim), dtype = 'float')\n",
        "    for head in range(num_heads):\n",
        "        _, k_enc, v_enc = calculate_qkv(enc_scores)\n",
        "        q_dec, _, _ = calculate_qkv(dec_scores)\n",
        "\n",
        "        attn_scores = self_attention(q_dec,k_enc,v_enc)\n",
        "        ## Copy scores to z_scores\n",
        "        start_idx = head*qkv_req_dim\n",
        "        end_idx = (head+1)*qkv_req_dim\n",
        "        z_scores[:,start_idx:end_idx] = attn_scores\n",
        "    \n",
        "    ## Iniitialize weight matrix to multiply with z_scores \n",
        "    w_o = np.random.rand(qkv_req_dim*num_heads, X.shape[1])\n",
        "    \n",
        "    multi_attn_score = dropout(np.matmul(z_scores, w_o))\n",
        "\n",
        "    return multi_attn_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7IbKRSZliF4"
      },
      "source": [
        "## Step 1.13\n",
        "Implement the decoder."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to initialize input to decoder. \n",
        "\n",
        "Input= start_tok_emb: start_token_embedding\n",
        "       max_len: max_length of tokens to be predicted by decoder. \n",
        "       emb_size: dimension of embedding vector.\n",
        "\n",
        "Output= init_dec_emb: Initial embedding for decoder. \n",
        "'''\n",
        "\n",
        "def init_dec_input(start_tok_emb,max_len,emb_size=512):\n",
        "    ## To adjust for start_tok_emb\n",
        "    init_dec_emb = np.random.rand(max_len+1,emb_size)\n",
        "    init_dec_emb[0] = start_tok_emb\n",
        "    \n",
        "    return init_dec_emb"
      ],
      "metadata": {
        "id": "N1TSpata40n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFcZ6Fb7liF5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function to create a single decoder. \n",
        "\n",
        "Input: word_emb: Word Embedding Matrix (Including Positional Encoding) or output from previous layer. \n",
        "       enc_scores : Final Scores from Encoder \n",
        "       num_heads: Number of attention heads. \n",
        "       qkv_req_dim: Dimenstion of query, key, value matrices (only number of columns)\n",
        "\n",
        "Output: Processed Word Embedding Matrix after passing through Decoder\n",
        "\n",
        "\n",
        "Steps:\n",
        "1. Masked Multihead Self-Attention Layer\n",
        "2. Residual Connection and LayerNorm (Add and Normalize)\n",
        "3. Encoder - Decoder Attention \n",
        "4. Residual Connection and LayerNorm (Add and Normalize)\n",
        "5. FFNN \n",
        "6. Residual Connection and LayerNorm (Add and Normalize)\n",
        "'''\n",
        "\n",
        "def decoder(word_emb, enc_scores, num_heads=8, qkv_req_dim=64):\n",
        "    #print(\"Input shape: \" + str(word_emb.shape))\n",
        "    ## can remove dec_query_list\n",
        "    dec_query_list, masked_attn_scores = decoder_self_attention(word_emb,num_heads,qkv_req_dim,mask=True)\n",
        "    #print(\"masked attn scores shape: \" + str(masked_attn_scores.shape))\n",
        "    layer_norm_scores1 = layer_norm(residual_connection(word_emb,masked_attn_scores))\n",
        "    #print(\"layer norm scores1 shape: \" + str(layer_norm_scores1.shape))\n",
        "    enc_dec_attn_scores = encoder_decoder_attention(word_emb, enc_scores, layer_norm_scores1, num_heads, qkv_req_dim, mask=False)\n",
        "    #print(\"Encoder Dec Scores shape: \" + str(enc_dec_attn_scores.shape))\n",
        "    layer_norm_scores2 = layer_norm(residual_connection(layer_norm_scores1,enc_dec_attn_scores))\n",
        "    #print(\"layer norm2 Scores shape: \" + str(layer_norm_scores2.shape))\n",
        "    ffnn_scores = ffnn(layer_norm_scores2)\n",
        "    #print(\"FFNN Scores shape: \" + str(ffnn_scores.shape))\n",
        "    final_scores = layer_norm(residual_connection(layer_norm_scores2,ffnn_scores))\n",
        "    #print(\"final scores shape: \" + str(final_scores.shape))\n",
        "    return final_scores \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Function to create decoder stack. \n",
        "\n",
        "Input = word_emb: Word Embedding Matrix (Including Positional Encoding)\n",
        "        stack_size: Number of encoders \n",
        "        enc_scores: Final Scores from Encoder.\n",
        "        num_heads: Number of attention heads. \n",
        "        qkv_req_dim: Dimenstion of query, key, value matrices (only number of columns)\n",
        "\n",
        "Output = Scores from Final Decoder Layer\n",
        "'''\n",
        "\n",
        "def decoder_stack(word_emb, enc_scores, stack_size = 6, num_heads=8, qkv_req_dim=64):\n",
        "    ## list to store encoder outputs. \n",
        "    decoder_op_list = []\n",
        "    \n",
        "    ## Output of first decoder \n",
        "    decoder_op1 = decoder(word_emb, enc_scores, num_heads, qkv_req_dim)\n",
        "    \n",
        "    decoder_op_list.append(decoder_op1)\n",
        "    #print(\"In decoder stack: \")\n",
        "    #print(\"Len decoder op1: \" + str(len(decoder_op1)))\n",
        "    \n",
        "    ## Pass final scores from previous encoders as encoder to next encoder. \n",
        "    for i in range(1,stack_size):\n",
        "        #print(\"In decoder stack: \" + str(i))\n",
        "        decoder_op = decoder(decoder_op_list[i-1], enc_scores, num_heads, qkv_req_dim)\n",
        "        decoder_op_list.append(decoder_op)\n",
        "    \n",
        "    return decoder_op_list"
      ],
      "metadata": {
        "id": "mg1ozftwGSZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKs2AGZ-liF5"
      },
      "source": [
        "## Step 1.14\n",
        "Implement the transformer architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPGw0p0kliGH"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Function to find closest embedding to decoder output. \n",
        "\n",
        "Input: candidate_vec = vector for finding closest embedding to. \n",
        "       target_matrix = matrix from which closest embedding needs to be calculated. \n",
        "       \n",
        "Output: Index of closest embedding in target_matrix\n",
        "'''\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def closest_embedding(candidate_vec,target_matrix):\n",
        "    cos_metric = []\n",
        "    for i in range(target_matrix.shape[0]):\n",
        "        cos_val = dot(candidate_vec, target_matrix[i])/(norm(candidate_vec)*norm(target_matrix[i]))\n",
        "        cos_metric.append(cos_val)\n",
        "    \n",
        "    return np.argmin(cos_metric)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. Token Embedding. \n",
        "2. Add Positional Encoding - Need to pass this to Encoder \n",
        "3. Encoder 1st part - Multi-head (8) Self Attention  - How to make parallize the inputs? (do we need threading?)\n",
        "4. Encoder 2nd part - LayerNorm (Residual Connection + O/p from Multihead self-attention)\n",
        "5. Encoder 3rd part - Pass output from 4 to FFNN. \n",
        "6. Repeat Residual Connection + O/p from Multihead self-attention. \n",
        "7. Make 6 blocks of Encoder and repeat 3-6. \n",
        "8. The output of 7 will be passed to each decoder. The Key and Vector matrix are fed to decoder (Need to check what goes exactly into decoder)\n",
        "9. Decoder 1st part - Multi-head (8) Masked Self Attention\n",
        "10. Decoder 2nd part - LayerNorm (Residual Connection + O/p from Multihead masked self-attention)\n",
        "11. Decoder 3rd part - Encoder - Decoder Attention\n",
        "12. Decoder 4th part - LayerNorm (Residual Connection + O/p from Enc-Dec self-attention)\n",
        "13. Decoder 5th part - Pass output from 12 to FFNN. \n",
        "14. Repeat Residual Connection + O/p from 13.\n",
        "15. 6 blocks of Decoder. \n",
        "16. Predict 1st word. \n",
        "17. Pass 16 as Decoder Input and repeat 9-16 for predicting subsequent words. \n",
        "18. Check how to add positional encoding to output of decoder.\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "PM95rwWoGYvL",
        "outputId": "1f4ff514-7725-421e-acea-94ab45aca9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. Token Embedding. \\n2. Add Positional Encoding - Need to pass this to Encoder \\n3. Encoder 1st part - Multi-head (8) Self Attention  - How to make parallize the inputs? (do we need threading?)\\n4. Encoder 2nd part - LayerNorm (Residual Connection + O/p from Multihead self-attention)\\n5. Encoder 3rd part - Pass output from 4 to FFNN. \\n6. Repeat Residual Connection + O/p from Multihead self-attention. \\n7. Make 6 blocks of Encoder and repeat 3-6. \\n8. The output of 7 will be passed to each decoder. The Key and Vector matrix are fed to decoder (Need to check what goes exactly into decoder)\\n9. Decoder 1st part - Multi-head (8) Masked Self Attention\\n10. Decoder 2nd part - LayerNorm (Residual Connection + O/p from Multihead masked self-attention)\\n11. Decoder 3rd part - Encoder - Decoder Attention\\n12. Decoder 4th part - LayerNorm (Residual Connection + O/p from Enc-Dec self-attention)\\n13. Decoder 5th part - Pass output from 12 to FFNN. \\n14. Repeat Residual Connection + O/p from 13.\\n15. 6 blocks of Decoder. \\n16. Predict 1st word. \\n17. Pass 16 as Decoder Input and repeat 9-16 for predicting subsequent words. \\n18. Check how to add positional encoding to output of decoder.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "Function to execute transformer architecture\n",
        "\n",
        "Input= forward_pass_array: input token array\n",
        "       src_vocab: source vocabulary \n",
        "       src_emb: source embedding\n",
        "       dec_max_len: decoding time steps to stop prediction.  \n",
        "       end_token: end token for decoder. \n",
        "'''\n",
        "\n",
        "def transformer(forward_pass_array, src_vocab=src_vocab, src_emb=src_emb, dec_max_len=10, end_token=715):\n",
        "    ## Get token embeddings. \n",
        "\n",
        "    ## input_token_emb Dimension = len(forward_pass_array)*512 \n",
        "    input_token_emb = generate_word_embedding(token_array=forward_pass_array, vocab=src_vocab, vocab_emb=src_emb)\n",
        "    input_seq_len = len(forward_pass_array)\n",
        "\n",
        "\n",
        "    ## Positional Encoding. Dimension = input_seq_len*512\n",
        "    enc_in_pos = positional_encoding(pos_encod_len=512, max_seq_len=input_seq_len)\n",
        "\n",
        "\n",
        "    ## Add token embedding and positional encoding. Dimension = input_seq_len*512\n",
        "    word_emb = input_token_emb + enc_in_pos\n",
        "\n",
        "\n",
        "    ## Encoder Blocks \n",
        "    encoder_output = encoder_stack(word_emb)\n",
        "\n",
        "\n",
        "    ## Output from last Encoder.\n",
        "    last_encoder_op = encoder_output[-1]\n",
        "    enc_query, enc_key, enc_value, enc_scores = last_encoder_op\n",
        "\n",
        "\n",
        "\n",
        "    ## Decoder Termination Criteria = 10 output words. \n",
        "    #dec_max_len = 10 \n",
        "\n",
        "    ## Intitialize start_tok_emb with dimension 512. \n",
        "    start_tok_emb = np.random.rand(512)\n",
        "\n",
        "    ## Decoder input token embedding\n",
        "    dec_in_token = init_dec_input(start_tok_emb,max_len=dec_max_len)\n",
        "\n",
        "    ## Positional Encoding \n",
        "    dec_in_pos = positional_encoding(pos_encod_len=512, max_seq_len=dec_max_len+1)\n",
        "\n",
        "    ## Add token embedding and positional encoding\n",
        "    dec_in = dec_in_token+dec_in_pos\n",
        "\n",
        "    predictions = []\n",
        "    for i in range(1,dec_max_len+1):\n",
        "        ## Decoder Blocks \n",
        "        decoder_output = decoder_stack(dec_in,enc_scores)\n",
        "\n",
        "        ## Output from last decoder. \n",
        "        dec_scores = decoder_output[-1]\n",
        "\n",
        "        ## Pass output to linear layer \n",
        "        linear_layer_scores = ffnn(dec_scores,n_hidden_layers=1,num_neuron_list=[512])\n",
        "\n",
        "        ## Pass linear_layers to softmax\n",
        "        softmax_scores = softmax(linear_layer_scores)\n",
        "\n",
        "        ## Check closest token embedding and token to softmax_scores at ith time step.\n",
        "        pred_token_arg = closest_embedding(softmax_scores[i], tgt_emb)\n",
        "\n",
        "        pred_token = tgt_vocab[pred_token_arg]\n",
        "\n",
        "        predictions.append(pred_token)\n",
        "        dec_in[i] = tgt_emb[pred_token_arg]\n",
        "\n",
        "        ## Add positional encodings in each time step.\n",
        "        dec_in += positional_encoding(pos_encod_len=512, max_seq_len=dec_max_len+1)\n",
        "\n",
        "        if pred_token==end_token:\n",
        "          break\n",
        "        \n",
        "    return predictions"
      ],
      "metadata": {
        "id": "lv0dYv7U5G83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVo5LvF1liGI"
      },
      "source": [
        "## Step 1.15\n",
        "Test the forward pass using the follow array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UCcXVfOliGJ",
        "outputId": "a2dd3486-a2c9-423d-cce9-f727b988e769",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[706, 700, 711, 707, 708, 702, 716, 715]\n"
          ]
        }
      ],
      "source": [
        "forward_pass_array = np.array([101, 400, 500, 600, 107, 102])\n",
        "output_predictions = transformer(forward_pass_array)\n",
        "print(output_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Testing with 20 random sequences. \n",
        "'''\n",
        "\n",
        "test_seq = [] \n",
        "for i in range(20):\n",
        "    arr = np.random.choice(src_vocab, size = (np.random.randint(low=1, high=len(src_vocab)-1)))\n",
        "    test_seq.append(arr)\n",
        "\n",
        "pred_test_seq = []\n",
        "\n",
        "for i in range(len(test_seq)):\n",
        "    transformer_input = test_seq[i]\n",
        "    pred_test_seq.append(transformer(transformer_input))\n",
        "    print(\"Input \" + str(i) + \": \" + str(test_seq[i]))\n",
        "    print(\"Output \" + str(i) + \": \" + str(pred_test_seq[i]))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZHbI0QH5M7s",
        "outputId": "17b77ac4-54e2-44f3-8651-4f820e12f66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input 0: [104 105 110 400 103 107 400]\n",
            "Output 0: [711, 700, 700, 705, 709, 713, 717, 719, 702, 710]\n",
            "\n",
            "\n",
            "Input 1: [400]\n",
            "Output 1: [718, 704, 709, 711, 719, 709, 710, 704, 702, 709]\n",
            "\n",
            "\n",
            "Input 2: [101 101 110 101 600 600]\n",
            "Output 2: [706, 708, 719, 711, 712, 711, 702, 714, 711, 707]\n",
            "\n",
            "\n",
            "Input 3: [101 104]\n",
            "Output 3: [714, 704, 716, 706, 700, 708, 715]\n",
            "\n",
            "\n",
            "Input 4: [108 105 107 105 110 108 107]\n",
            "Output 4: [710, 717, 713, 700, 706, 717, 700, 700, 705, 700]\n",
            "\n",
            "\n",
            "Input 5: [105 105 500]\n",
            "Output 5: [701, 710, 709, 705, 711, 700, 706, 707, 707, 703]\n",
            "\n",
            "\n",
            "Input 6: [103 105 400]\n",
            "Output 6: [704, 700, 713, 716, 714, 711, 700, 713, 701, 717]\n",
            "\n",
            "\n",
            "Input 7: [109 109 106 109 400 103]\n",
            "Output 7: [715]\n",
            "\n",
            "\n",
            "Input 8: [105 108 108 104 104 107 102]\n",
            "Output 8: [703, 705, 707, 713, 705, 700, 705, 708, 701, 716]\n",
            "\n",
            "\n",
            "Input 9: [103]\n",
            "Output 9: [716, 715]\n",
            "\n",
            "\n",
            "Input 10: [104 101 400 108 108 103 101 400 102]\n",
            "Output 10: [707, 715]\n",
            "\n",
            "\n",
            "Input 11: [105 108 108 400 106 108 110 102 109 105]\n",
            "Output 11: [715]\n",
            "\n",
            "\n",
            "Input 12: [110 104 108 600 109 104]\n",
            "Output 12: [706, 715]\n",
            "\n",
            "\n",
            "Input 13: [106 104 104 600 103 104 107 107]\n",
            "Output 13: [702, 701, 704, 708, 714, 710, 711, 714, 700, 717]\n",
            "\n",
            "\n",
            "Input 14: [103 110 106 104 400 106]\n",
            "Output 14: [711, 718, 700, 719, 712, 715]\n",
            "\n",
            "\n",
            "Input 15: [400]\n",
            "Output 15: [710, 704, 706, 705, 707, 712, 700, 707, 703, 704]\n",
            "\n",
            "\n",
            "Input 16: [101 106 400 105 102 108 400]\n",
            "Output 16: [712, 708, 716, 704, 709, 700, 713, 704, 711, 713]\n",
            "\n",
            "\n",
            "Input 17: [106 108 102 600 107 106 600]\n",
            "Output 17: [716, 712, 701, 716, 712, 706, 714, 709, 708, 719]\n",
            "\n",
            "\n",
            "Input 18: [109 101 104]\n",
            "Output 18: [706, 715]\n",
            "\n",
            "\n",
            "Input 19: [105 110 101 107 103 400 101 500 107 600]\n",
            "Output 19: [708, 719, 706, 709, 718, 712, 713, 700, 704, 709]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qUbYT2PPliGP",
        "bh4sr-BFliGP",
        "_3pzKUR9liGQ"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}